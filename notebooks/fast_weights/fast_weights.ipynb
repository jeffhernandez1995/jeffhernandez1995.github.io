{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the claims of the fast weights paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use fast weights to aid in learning associative tasks and store temporary memories of recent past. In a traditional recurrent architecture we have our slow weights which are used to determine the next hidden state and hold long-term memory. We introduce the concept of fast weights, in conjunction with the slow weights, in order to account for short-term knowledge. These weights are quick to update and decay as they change from the introduction of new hidden states.\n",
    "\n",
    "They extend the standard vanilla recurrent neural network architecture with some form of Hebbian short-term synaptic plasticity. This Hebbian connectivity maintains a dynamically changing short-term memory of the recent history of the activities of the units in the network. They call this Hebbian connectivity “fast weights” as opposed to the standard “slow” recurrent connectivity.\n",
    "\n",
    "We don't store memories by keeping track of the exact neural activity that occurred at the time of the memory. Instead, we try to recreate the neural activity through a set of associative weights which can map to many other memories as well. This allows for efficient storage of many memories without storing separate weights for each instance. This associative network also allows for associative learning which is the ability to learn the recall the relationship between initially unrelated instances.\n",
    "\n",
    "## Concept\n",
    "* In a traditional recurrent architecture we have our slow weights. These weights are used with the input and the previous hidden state to determine the next hidden state. These weights are responsible for the long-term knowledge of our systems. These weights are updated at the end of a batch, so they are quite slow to update and decay.\n",
    "\n",
    "* They introduce the concept of fast weights, in conjunction with the slow weights, in order to account for short-term knowledge. These weights are quick to update and decay as they change from the introduction of new hidden states.\n",
    "\n",
    "\n",
    "The equation governing the model can be condensed into the following form (The inner loop of Hebbian plasticity is unrolled for one step only, i.e. S=1 as done in the paper)\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_{t+1} = f\\left(\\mathcal{LN}\\left[\\mathbf{W}_h \\mathbf{h}_{t} + \\mathbf{W}_x \\mathbf{x}_{t} + (\\eta \\sum_{\\tau=1}^{\\tau=t-1} \\lambda^{t - \\tau -1} f(\\mathbf{W}_h \\mathbf{h}_{t} +  \\mathbf{W}_x \\mathbf{x}_{t})\\right]\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['replication_1/', 'replication_2/', 'replication_3/']\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "replications = sorted(glob.glob('replication_*/'))\n",
    "print(replications)\n",
    "all_exps = []\n",
    "for rep in replications:\n",
    "    temp_exp = sorted(glob.glob(os.path.join(rep, 'exps/*/*_testinglog.npy')))\n",
    "    print(len(temp_exp))\n",
    "    all_exps.append(temp_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_file = np.load('replication_1/exps/exp_0_0_1_1/exp_0_0_1_1_testinglog.npy', allow_pickle=True)[0]\n",
    "# ce = exp_file['CrossEntropy']\n",
    "# max_val = np.array(ce).max()\n",
    "# max_id = np.argmax(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "exp_arr = np.zeros((3, 10, 2, 200))\n",
    "for rep in range(len(replications)):\n",
    "    labels = []\n",
    "    for e, exp in enumerate(all_exps[rep]):\n",
    "        exp_file = np.load(exp, allow_pickle=True)[0]\n",
    "        length_ce = len(exp_file['CrossEntropy'])\n",
    "        length_acc = len(exp_file['Accuracy'])\n",
    "        assert length_acc == length_ce\n",
    "        if length_ce > 200:\n",
    "            length = 200\n",
    "        else:\n",
    "            length = length_ce\n",
    "        exp_arr[rep, e, 0, :length] = exp_file['CrossEntropy'][:length]\n",
    "        exp_arr[rep, e, 1, :length] = exp_file['Accuracy'][:length]\n",
    "        # parse exp name\n",
    "        exp_name = exp.split('/')[-2]\n",
    "#         if exp_name == 'exp_0_0_1_1':\n",
    "#             ce[max_id] = 0.5*(ce[max_id-1] + ce[max_id+1])\n",
    "        exp_parms = [int(f) for f in exp_name.split('_')[1:]]\n",
    "        label = 'RNN-'\n",
    "        if exp_parms[0] == 1:\n",
    "            label += 'CTRL-'\n",
    "        else:\n",
    "            label += 'FW-'\n",
    "        if exp_parms[1] == 1:\n",
    "            label += 'LN-'\n",
    "        if exp_parms[2] == 1:\n",
    "            label += 'DEPTH-'\n",
    "        if exp_parms[3] == 0:\n",
    "            label += 'HS=64'\n",
    "        else:\n",
    "            label += 'HS=128'\n",
    "        labels.append(label)\n",
    "exp_arr = np.transpose(exp_arr, (1, 0, 2, 3))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3503e0cf83ed43fb992ea440ded4e4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a226620ba94827b00ffde4cbee7f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1e555c1b38>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(8, 4)) # figsize=(10, 10)\n",
    "fig2, ax2 = plt.subplots(figsize=(8, 4)) # figsize=(10, 10)\n",
    "for l, label in enumerate(labels):\n",
    "    df_acc = pd.DataFrame(exp_arr[l, :, 1, :].T, columns=replications)\n",
    "    for col in replications:\n",
    "        df_acc[col] = df_acc[col].replace(0, np.nan).ffill()\n",
    "    df_acc['median'] = df_acc.quantile(0.5, axis=1)\n",
    "    df_acc['1_quartil'] = df_acc.quantile(0.10, axis=1)\n",
    "    df_acc['3_quartil'] = df_acc.quantile(0.90, axis=1)\n",
    "#     print(df_acc.head())\n",
    "#     assert 2 == 1\n",
    "    df_ce = pd.DataFrame(exp_arr[l, :, 0, :].T, columns=replications)\n",
    "    for col in replications:\n",
    "        df_ce[col] = df_ce[col].replace(0, np.nan).ffill()\n",
    "    df_ce['median'] = df_ce.median(axis=1)\n",
    "    df_ce['1_quartil'] = df_ce.quantile(0.10, axis=1)\n",
    "    df_ce['3_quartil'] = df_ce.quantile(0.90, axis=1)\n",
    "    \n",
    "    ax1.fill_between(\n",
    "        np.arange(1, 201),\n",
    "        df_acc['1_quartil'].values,\n",
    "        df_acc['3_quartil'].values,\n",
    "        alpha=0.2\n",
    "    )\n",
    "    ax1.plot(np.arange(1, 201), df_acc['median'].values, label=label)\n",
    "\n",
    "    ax2.fill_between(\n",
    "        np.arange(1, 201),\n",
    "        df_ce['1_quartil'].values,\n",
    "        df_ce['3_quartil'].values,\n",
    "        alpha=0.2\n",
    "    )\n",
    "    ax2.plot(np.arange(1, 201), df_ce['median'].values, label=label)\n",
    "ax1.set_xlabel('No. of Epochs')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax2.set_ylim(bottom=0)\n",
    "ax2.set_xlim(left=0)\n",
    "ax1.legend(frameon=True)\n",
    "# fig1.savefig('acc.svg', bbox_inches='tight')\n",
    "ax2.set_xlabel('No. of Epochs')\n",
    "ax2.set_ylabel('Cross Entropy')\n",
    "ax2.set_ylim(bottom=0)\n",
    "ax2.set_xlim(left=0)\n",
    "ax2.legend(frameon=True)\n",
    "# fig2.savefig('ce.svg', bbox_inches='tight')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.41770833333334\n"
     ]
    }
   ],
   "source": [
    "ctrl_acc = 0\n",
    "for rep in range(len(replications)):\n",
    "    # RNN-CTRL-HS=64\n",
    "    ctrl_1 = np.load(\n",
    "        f'replication_{rep+1}/exps/exp_1_0_0_1/exp_1_0_0_1_testinglog.npy',\n",
    "        allow_pickle=True)[0]\n",
    "    ctrl_1_best_acc = np.array(ctrl_1['Accuracy'][:200]).max()\n",
    "    # RNN-CTRL-HS=128\n",
    "    ctrl_2 = np.load(\n",
    "        f'replication_{rep+1}/exps/exp_1_0_0_0/exp_1_0_0_0_testinglog.npy',\n",
    "        allow_pickle=True)[0]\n",
    "    ctrl_2_best_acc = np.array(ctrl_2['Accuracy'][:200]).max()\n",
    "    ctrl_acc += (1 / len(replications)) * 0.5 * (ctrl_1_best_acc + ctrl_2_best_acc)\n",
    "print(ctrl_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1, -1, -1], [-1, -1, 1], [-1, 1, -1], [-1, 1, 1], [1, -1, -1], [1, -1, 1], [1, 1, -1], [1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "results = list(itertools.product([-1,1],[-1,1],[-1,1]))\n",
    "results = [list(e) for e in results]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1, -1, -1, 1.0008310578178796, 0.9764590152812875, 0.9412493003850001], [-1, -1, 1, 1.038126897440681, 1.1835450552059834, 0.9575312494699885], [-1, 1, -1, 1.2640898220857855, 1.2580349722698054, 1.2423127151846134], [-1, 1, 1, 1.4209562253014703, 1.4224317769372974, 1.406149827852309], [1, -1, -1, 0.8251386509726767, 0.8756635742270313, 0.7041434168348569], [1, -1, 1, 1.2926341140754056, 1.329828191516426, 1.0130934007225114], [1, 1, -1, 1.0021030850901442, 1.4850663998236122, 1.5310629059887042], [1, 1, 1, 1.6013296925086073, 1.578992893607639, 1.5873882736045861]]\n"
     ]
    }
   ],
   "source": [
    "for rep in replications:\n",
    "    for i, enc_inp in enumerate(results):\n",
    "        enc_inp = np.array(enc_inp).astype(int)\n",
    "        enc_inp[enc_inp==-1] = 0\n",
    "        exp_name = f'exp_0_{enc_inp[0]}_{enc_inp[1]}_{enc_inp[2]}'\n",
    "        name = f'{rep}/exps/{exp_name}/{exp_name}_testinglog.npy'\n",
    "        exp_file = np.load(name, allow_pickle=True)[0]\n",
    "        exp_acc =  np.array(exp_file['Accuracy'][:200]).max()\n",
    "#         exp_acc = np.log(exp_acc / ctrl_acc)\n",
    "        exp_acc = exp_acc / ctrl_acc\n",
    "        results[i].append(exp_acc)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LN  DE  HS     rep_0     rep_1     rep_2     y_avg\n",
      "0  -1  -1  -1  1.000831  0.976459  0.941249  0.972846\n",
      "1  -1  -1   1  1.038127  1.183545  0.957531  1.059734\n",
      "2  -1   1  -1  1.264090  1.258035  1.242313  1.254813\n",
      "3  -1   1   1  1.420956  1.422432  1.406150  1.416513\n",
      "4   1  -1  -1  0.825139  0.875664  0.704143  0.801649\n",
      "5   1  -1   1  1.292634  1.329828  1.013093  1.211852\n",
      "6   1   1  -1  1.002103  1.485066  1.531063  1.339411\n",
      "7   1   1   1  1.601330  1.578993  1.587388  1.589237\n",
      "[[ 0.0279846   0.00361256 -0.03159716]\n",
      " [-0.0216075   0.12381065 -0.10220315]\n",
      " [ 0.00927732  0.00322247 -0.01249979]\n",
      " [ 0.00444362  0.00591917 -0.01036278]\n",
      " [ 0.0234901   0.07401503 -0.09750513]\n",
      " [ 0.08078221  0.11797629 -0.1987585 ]\n",
      " [-0.33730771  0.1456556   0.19165211]\n",
      " [ 0.01209274 -0.01024406 -0.00184868]]\n"
     ]
    }
   ],
   "source": [
    "labels = [\n",
    "    'LN',\n",
    "    'DE',\n",
    "    'HS'\n",
    "]\n",
    "rep_cols = []\n",
    "for rep in range(len(replications)):\n",
    "    rep_cols.append(f'rep_{rep}')\n",
    "col_names = labels + rep_cols\n",
    "results_df = pd.DataFrame(results, columns=col_names)\n",
    "results_df['y_avg'] = results_df[rep_cols].mean(axis=1)\n",
    "err_df = results_df[rep_cols].values - np.repeat(results_df['y_avg'].values, 3).reshape(8, 3)\n",
    "print(results_df)\n",
    "print(err_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One way effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LN': 0.05956055697834173, 'DE': 0.3884728888587372, 'HS': 0.22715439019012584}\n"
     ]
    }
   ],
   "source": [
    "effects = {}\n",
    "for key in labels:\n",
    "    eff = results_df.groupby(key)['y_avg'].mean()\n",
    "    effects[key] = sum([i*eff[i] for i in [-1,1]])\n",
    "print(effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two way effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LN': 0.05956055697834173, 'DE': 0.3884728888587372, 'HS': 0.22715439019012584, 'LN-DE': 0.06910076152032685, 'LN-HS': 0.10286036532623266, 'DE-HS': -0.021391258628585152}\n"
     ]
    }
   ],
   "source": [
    "twoway_labels = list(itertools.combinations(labels, 2))\n",
    "for key in twoway_labels:\n",
    "    eff = results_df.groupby(list(key))['y_avg'].mean()\n",
    "    effects['-'.join(key)] = sum([i*j*eff[i][j]/2 for i in [-1,1] for j in [-1,1]])\n",
    "print(effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three way effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LN': 0.05956055697834173, 'DE': 0.3884728888587372, 'HS': 0.22715439019012584, 'LN-DE': 0.06910076152032685, 'LN-HS': 0.10286036532623266, 'DE-HS': -0.021391258628585152, 'LN-DE-HS': -0.05879734061498276}\n"
     ]
    }
   ],
   "source": [
    "threeway_labels = list(itertools.combinations(labels, 3))\n",
    "for key in threeway_labels: \n",
    "    eff = results_df.groupby(list(key))['y_avg'].mean()\n",
    "    effects['-'.join(key)] = sum([i*j*k*eff[i][j][k]/4 for i in [-1,1] for j in [-1,1] for k in [-1,1]])\n",
    "print(effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraction of variation explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7039317461837857\n",
      "2.979846171498194\n",
      "0.01724465158215053\n",
      "0.0268053567268983\n"
     ]
    }
   ],
   "source": [
    "SST = 4 * len(replications) * sum([value**2 for key, value in effects.items()])\n",
    "print(SST)\n",
    "SST += (err_df **2).sum()\n",
    "print(SST)\n",
    "sigma = (err_df **2).sum() / (8 * 2)\n",
    "print(sigma)\n",
    "s_q_i = np.sqrt(sigma / (8 * 3))\n",
    "t_95_16 = 1.7459\n",
    "print(s_q_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16236048990371632\n"
     ]
    }
   ],
   "source": [
    "I = results_df['y_avg'].mean()\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           effects       %var  lower_CI  upper_CI\n",
      "I         0.162360   0.000000  0.115561  0.209160\n",
      "LN        0.059561   1.428581  0.012761  0.106360\n",
      "DE        0.388473  60.772742  0.341673  0.435272\n",
      "HS        0.227154  20.779241  0.180355  0.273954\n",
      "LN-DE     0.069101   1.922884  0.022301  0.115900\n",
      "LN-HS     0.102860   4.260725  0.056061  0.149660\n",
      "DE-HS    -0.021391   0.184272 -0.068191  0.025408\n",
      "LN-DE-HS -0.058797   1.392204 -0.105597 -0.011998\n",
      "e         0.000000   9.259351  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "row_names = ['I'] + list(effects.keys()) + ['e']\n",
    "effects_ = np.zeros(len(list(effects.keys())) + 2)\n",
    "variations = np.zeros(len(list(effects.keys())) + 2)\n",
    "lower = np.zeros(len(list(effects.keys())) + 2)\n",
    "upper = np.zeros(len(list(effects.keys())) + 2)\n",
    "effects_[0] = I\n",
    "effects_[1:-1] = np.array(list(effects.values()))\n",
    "variations[1:-1] = 4 * len(replications)*  ((effects_[1:-1]**2 ) / SST) * 100\n",
    "variations[-1] = ((err_df **2).sum() / SST) * 100\n",
    "lower = effects_ - t_95_16 * s_q_i\n",
    "lower[-1] = 0\n",
    "upper = effects_ + t_95_16 * s_q_i\n",
    "upper[-1] = 0\n",
    "arr = np.array([effects_, variations, lower, upper]).T\n",
    "experiment_df = pd.DataFrame(arr, columns=['effects', '%var', 'lower_CI', 'upper_CI'], index=row_names)\n",
    "print(experiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f70bad69fd4af9ac6d788284b65296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# import numpy as np\n",
    "\n",
    "x = np.linspace(0,1,50)\n",
    "y = np.linspace(0,1,50)\n",
    "z = np.linspace(0,1,50)\n",
    "\n",
    "Z = np.outer(z.T, z)        # 50x50\n",
    "X, Y = np.meshgrid(x, y)    # 50x50\n",
    "\n",
    "color_dimension = 0.162360 + 0.059561*X + 0.3884*Y + 0.227154*Z + \\\n",
    "    0.069101*X*Y + 0.102860*X*Z - 0.021391*Y*Z -0.05879*X*Y*Z\n",
    "color_dimension /= 2\n",
    "minn, maxx = color_dimension.min(), color_dimension.max()\n",
    "norm = matplotlib.colors.Normalize(minn, maxx)\n",
    "m = plt.cm.ScalarMappable(norm=norm, cmap='coolwarm')\n",
    "m.set_array([])\n",
    "fcolors = m.to_rgba(color_dimension)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "# fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X,Y,Z, rstride=1, cstride=1, facecolors=fcolors, vmin=minn, vmax=maxx, shade=False)\n",
    "ax.set_xlabel('LN')\n",
    "ax.set_ylabel('DE')\n",
    "ax.set_zlabel('HS')\n",
    "plt.show()\n",
    "# fig.savefig('surface.svg', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
