---
title: "Link Collection 2"
author: Jefferson Hern√°ndez
categories: links, collection, ml
date:   2024-01-13 10:50:00
use_math: true
---

Some links and papers that I have found interesting this week. If you have any comments, please let me know.

**1:** [Finetune Mistral-7B using qLora](https://github.com/brevdev/notebooks/blob/main/mistral-finetune.ipynb) This is a notebook authored by [Brevdev](https://github.com/brevdev) that shows how to finetune the Mistral-7B model using qLora. The notebook is very well documented and easy to follow. I have not tried it yet, but I will do it soon.

**2:** [A tour of parallelism in JAX](https://colab.research.google.com/drive/1uXrhGCHYZjMwtu_wIvvUjNq9nBE2MZUj?usp=sharing) I have been trying to learn JAX for some time now, and I have found this notebook very useful. It is a very good introduction to parallelism in JAX.

**3:** [Use GPT-4V for data labeling](https://github.com/roboflow/multimodal-maestro) This repository authored by Roboflow shows how to use GPT-4V for data labeling. The repository is very well documented and easy to follow. The repo is in an early stage but I think it is a very interesting idea. See the following image for and example of the results.
![Multimodal Maestro](https://i.imgur.com/A6hvhFq.png)

**4:** [GPT-fast](https://pytorch.org/blog/accelerating-generative-ai-2/) This blogpost by the PyTorch team shows how to use pure PyTorch to accelerate inference of LLMs using Torch.compile, GPU quantization Speculative Decoding and Tensor Parallelism. The results are very impressive almost 10x faster than the baseline. See the following image for the results.
![LlaMA fast inference](https://pytorch.org/assets/images/accelerating-generative-ai-2/screen-recording.gif).

**5:** [Extending the context of LLMs](https://www.reddit.com/r/LocalLLaMA/comments/194mmki/selfextend_works_for_phi2_now_looks_good/?utm_source=share&utm_medium=web2x&context=3) This reddit post on the r/LocalLLaMA subreddit shows how to extend the context of LLMs using the self-extend method. The results are very interesting. See the following image for the results for the Phi-2 model.
![Phi-2 self-extend](https://preview.redd.it/0ij1edf0wxbc1.jpg?width=1324&format=pjpg&auto=webp&s=845087a0bf5c30a4c5fef01345bd178d0b808167)

**6:** [LLM+CLIP for image captioning](https://colab.research.google.com/drive/1jewRcVdPybX4_M41jLcoyniTcDmnVGTd#scrollTo=6_k53cVFyAQe) This notebook authored by [Katherine Crowson](https://twitter.com/RiversHaveWings) shows how to use LLM+CLIP for image captioning. The idea of using gradient descent and PEFT to find the caption that most closely matches the CLIP image embeddings is very interesting, and the results are very suprisingly good, but I think that doing SGD for every image is not very efficient.

{% include disqus.html %}