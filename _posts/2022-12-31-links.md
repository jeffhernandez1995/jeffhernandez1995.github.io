---
title: "Link Collection 1"
author: Jefferson Hern√°ndez
categories: links, collection, ml
date:   2023-12-31 10:50:00
use_math: true
---

Some links and papers that I have found interesting this week. If you have any comments, please let me know.

**1:** [Perspectives on the State and Future of Deep Learning - 2023](https://arxiv.org/pdf/2312.09323.pdf), a very good paper that summarizes the current state of deep learning and the challenges that it faces in the future. It is a long read, but it is worth it. They interview Kyunghyun Cho, Zachary C Lipton, Melanie Mitchell, Preetum Nakkiran, Max Welling, Andrew Gordon Wilson on five pressing questions about the future of deep learning.

**2:** [Deep Learning Tuning Playbook](https://github.com/google-research/tuning_playbook) is a very good resource for maximizing the performance of deep learning models. It is a collection of best practices and tips for tuning deep learning models.

**3:** [Choose Your Weapon: Survival Strategies for Depressed AI Academics](https://arxiv.org/pdf/2304.06035.pdf) by Julian Togelius and Georgios N. Yannakakis is a kind of satirical paper about the current state of AI research and what small labs can do to stay competitive while remanining academic. It is a very good read.

**4:** [A PhD in Numbers](https://davidstutz.de/a-phd-in-numbers/) by David Stutz is a very good post about his journey through his PhD. He talks about his experience, the challenges he faced and the lessons he learned and what he accomlished each year.

**5:** [Role-Playing Paper-Reading Seminars](https://colinraffel.com/blog/role-playing-seminar.html) by Alec Jacobson and Colin Raffel. I just found this method of running a paper reading seminar very interesting. I have only partipated in regular seminar during my PhD, but I think this method could be very useful to get more out of the papers.

**6:** [The Essense of Global Convolution Models](https://benathi.github.io/blogs/2023-12/global-convolution-models/) by Ben Athiwaratkun is a very good technical post about global convolution models. He explains the intuition behind them and how they work and help me understam the new [Mamba model](https://arxiv.org/pdf/2312.00752.pdf). He also provides a very good list of references.

**7:** Some papers I read this week and found interesting: [Exploiting Inductive Biases in Video Modeling through Neural CDEs](https://arxiv.org/pdf/2311.04986.pdf), [Quantifying & Modeling Multimodal Interactions:
An Information Decomposition Framework](https://arxiv.org/pdf/2302.12247.pdf), [To Compress or Not to Compress - Self-Supervised Learning and Information Theory: A Review](https://arxiv.org/pdf/2304.09355.pdf)

{% include disqus.html %}