---
title: ""
permalink: /
redirect_from: 
  - /about/
  - /about.html
---

<h2 class="about-title">About Me</h2>
<div class="about-hero">
  <div class="about-hero__image">
    <img src="{{ '/pictures/me.png' | relative_url }}" alt="Portrait of Jefferson Hernandez">
  </div>
  <div class="about-hero__text" markdown="1">
Hola! I am **Jefferson Hernandez**, a PhD student in Computer Science at [Rice University](https://cs.rice.edu/), working in self-supervised learning for multimodal data and reasoning in Large language models under the supervision of [Prof. Vicente Ordonez](http://vicenteordonez.com) at [Vislang Lab](https://www.vislang.ai).

I am currently a Research Intern at [Meta Reality Labs](https://tech.facebook.com/reality-labs/) working with [Ishwarya Ananthabhotla](https://www.ishwarya.me/). I previously interned at [Adobe Research](https://research.adobe.com/) working with [Kushal Kafle](https://kushalkafle.com/). I have also colaborated with [Ruben Villegas](https://rubenvillegas.me/) on self-supervised learning for video data.

Prior to this, I obtained my bachelor's degree in Industrial Engineering from [ESPOL](http://www.espol.edu.ec/) (top 1% of the class), where I worked with [Prof. Andres G. Abad](https://www.researchgate.net/profile/Andres-Abad-2) on machine learning and computer vision. I also worked as a research assistant at [INARI Lab](https://inarilab.com/) on applications of computer vision to retail. I have also shortly worked as a Computer Vision engineer at [adaviv](https://www.adaviv.com/).

I am interested in computer vision, natural language processing, and machine learning. I am particularly interested in test-time-training for LLMs and self-supervised learning for images, text, video and audio. As a PhD student, I am always eager to collaborate with other researchers. If you are interested in working with me, feel free to reach out to me via email.
  </div>
</div>

### üî• News
<hr>
- *2025.06*: &nbsp;üìÑ [New Work] **[GViT](https://arxiv.org/abs/2506.23532)** is on arXiv.
- *2025.05*: &nbsp;üìÑ [New Work] **[ProxyThinker](https://arxiv.org/abs/2505.24872)** is on arXiv.
- *2025.03*: &nbsp;üéâüéâ **[cFreD](https://arxiv.org/abs/2503.21721)** is accepted at WACV 2026.
- *2024.11*: &nbsp;üéâüéâ **[Panel-of-Peers](https://arxiv.org/abs/2509.01610)** is accepted at ICCV 2025.
- *2024.06*: &nbsp;üìÑ [New Work] **[GenLLaVA](https://arxiv.org/abs/2406.11262)** is on arXiv.
- *2023.11*: &nbsp;üéâüéâ **[ViC-MAE](https://jeffhernandez1995.github.io/vicmae/)** accepted at ECCV 2024.
- *2022.08*: &nbsp;üöÄ Started PhD journey at Rice University under Prof. Vicente Ordonez.

### üìù Preprints
<hr>
<div class='paper-box'><div class="paper-box-image"><div><img src="{{ '/pictures/gvit_teaser.png' | relative_url }}" alt="GVIT teaser" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>GViT: Representing Images as Gaussians for Visual Recognition</b>. [[arxiv]](https://arxiv.org/abs/2506.23532) <br />
<b>Jefferson Hernandez</b>, Ruozhen He, Guha Balakrishnan, Alexander C. Berg, Vicente Ordonez <br />
June 2025 <br />
</div>
</div>

<div class='paper-box'><div class="paper-box-image"><div><img src="{{ '/pictures/proxythinker_teaser.png' | relative_url }}" alt="ProxyThinker teaser" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>ProxyThinker: Test-Time Guidance through Small Visual Reasoners</b>. [[arxiv]](https://arxiv.org/abs/2505.24872) <br />
Zilin Xiao, Jaywon Koo, Siru Ouyang, <b>Jefferson Hernandez</b>, Yu Meng, Vicente Ordonez <br />
May 2025 <br />
</div>
</div>

<div class='paper-box'><div class="paper-box-image"><div><img src="{{ '/pictures/genllava_teaser.png' | relative_url }}" alt="GenLLaVA teaser" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>Generative Visual Instruction Tuning</b>. [[arxiv]](https://arxiv.org/abs/2406.11262) <br />
<b>Jefferson Hernandez</b>, Ruben Villegas and Vicente Ordonez <br />
June 2024 <br />
</div>
</div>

### üìù Publications 
<hr>
<div class='paper-box'><div class='paper-box-image'><div><img src="{{ '/pictures/cfred_teaser.png' | relative_url }}" alt="cFreD teaser" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>Evaluating Text-to-Image Synthesis with a Conditional Fr√©chet Distance</b> [[arxiv]](https://arxiv.org/abs/2503.21721) [[Code]](https://github.com/JaywonKoo17/FIDMetric)<br />
Jaywon Koo*, <b>Jefferson Hernandez*</b>, Moayed Haji-Ali, Ziyan Yang, Vicente Ordonez <br />
*Equal contribution. March 2025, WACV 2026 <br />
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src="{{ '/pictures/pop_teaser.png' | relative_url }}" alt="cFreD teaser" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>Improving Large Vision and Language Models by Learning from a Panel of Peers</b> [[arxiv]](https://arxiv.org/abs/2509.01610)<br />
 <b>Jefferson Hernandez</b>, Jing Shi, Simon Jenni, Vicente Ordonez, Kushal Kafle <br />
November 2024, ICCV 2025 <br />
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src="{{ '/pictures/vic_mae_teaser.png' | relative_url }}" alt="ViC-MAE teaser" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>ViC-MAE: Self-Supervised Representation Learning from Images and Video </b>
 <br />
<b>with Contrastive Masked Autoencoders</b>. [[Paper]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00629.pdf) [[arxiv]](https://arxiv.org/abs/2303.12001) [[Project Page]](https://jeffhernandez1995.github.io/vicmae/) [[Code]](https://github.com/jeffhernandez1995/ViC-MAE)<br />
<b>Jefferson Hernandez</b>, Ruben Villegas and Vicente Ordonez <br />
November 2023, ECCV 2024 <br />
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src="{{ '/pictures/retail_sync.png' | relative_url }}" alt="Retail Dataset" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>Automatic Retail Dataset Creation with Multiple Sources of Information Synchronization</b>. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10320058) <br />
Ricardo Palacios, Byron Piguave, <b>Jefferson Hernandez</b>,and Andres Abad <br />
October 2023, IPTA 2023 <br />
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src="{{ '/pictures/action_recognition.png' | relative_url }}" alt="Action Recognition" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>A View Invariant Human Action Recognition System for Noisy Inputs</b>. [[Paper]](https://ieeexplore.ieee.org/abstract/document/9867004) <br />
<b>Jefferson Hernandez</b>, J.W. Kim, Ruben Cobos, and Andres Abad <br />
May 2022, CRV 2022 <br />
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src="{{ '/pictures/hierarchical_har.png' | relative_url }}" alt="Hierarchical HAR" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>Hierarchical Human Action Recognition to Measure the Performance of Manual Labor</b>. [[Paper]](https://ieeexplore.ieee.org/abstract/document/9482414) <br />
<b>Jefferson Hernandez</b>, Gabriela Valarezo, Ruben Cobos, J.W. Kim, and Andres Abad <br />
2021, IEEE Access <br />
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src="{{ '/pictures/timemotion_study.png' | relative_url }}" alt="Time Motion Study" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>Automatic Time and Motion Study Using Deep Learning</b>. [[Paper]](https://www.taylorfrancis.com/chapters/edit/10.1201/9781003146711-10/automatic-time-motion-study-using-deep-learning-jefferson-hernandez-sofia-lopez-gabriela-valarezo-andres-abad) <br />
<b>Jefferson Hernandez</b>, Sofia Lopez, Gabriela Valarezo, and Andres Abad <br />
2021, CRC Press <br />
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src="{{ '/pictures/multi_obj_tracking.png' | relative_url }}" alt="Multi Object Tracking" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>A fast multi-object tracking system using an object detector ensemble</b>. [[Paper]](https://ieeexplore.ieee.org/document/8781972) [[arxiv]](https://arxiv.org/abs/1908.04349) <br />
<b>Jefferson Hernandez</b>, Ruben Cobos, and Andres Abad <br />
June 2019, ColCACI 2019 <br />
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src="{{ '/pictures/retail_traffic.png' | relative_url }}" alt="Retail Traffic Flow" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>Retail Traffic-Flow Analysis Using a Fast Multi-object Detection and Tracking System</b>. [[Paper]](https://link.springer.com/chapter/10.1007%2F978-3-030-36211-9_3) <br />
<b>Jefferson Hernandez</b>, Ruben Cobos, and Andres Abad <br />
2019, Springer, ColCACI <br />
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src="{{ '/pictures/rbm_multivariate.png' | relative_url }}" alt="RBM Model" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>Learning from multivariate discrete sequential data </b>
 <br />
 <b> using a restricted Boltzmann machine model</b>. [[Paper]](https://ieeexplore.ieee.org/abstract/document/8484854) [[arxiv]](https://arxiv.org/abs/1804.10839) <br />
<b>Jefferson Hernandez</b>, and Andres Abad <br />
May 2018, ColCACI 2018 <br />
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src="{{ '/pictures/rbm_features.png' | relative_url }}" alt="RBM Features" width=150px></div></div>
<div class='paper-box-text' markdown="1">
<b>Spatial and Temporal Feature Extraction Using a Restricted Boltzmann Machine Model</b>. [[Paper]](https://link.springer.com/chapter/10.1007%2F978-3-030-03023-0_1) <br />
<b>Jefferson Hernandez</b>, and Andres Abad <br />
May 2018, Springer, ColCACI <br />
</div>
</div>
